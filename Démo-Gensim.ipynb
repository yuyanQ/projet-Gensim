{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c9fdfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document  1 :  Bonjour, pouvez-vous m'aider s'il vous plaît ?\n",
      "Sujets : \n",
      " - Sujet  1 : probabilité= 0.07259065\n",
      " - Sujet  2 : probabilité= 0.92740935\n",
      "Document  2 :  Bien sûr, que puis-je faire pour vous ?\n",
      "Sujets : \n",
      " - Sujet  1 : probabilité= 0.93678296\n",
      " - Sujet  2 : probabilité= 0.063217066\n",
      "Document  3 :  Je rencontre un problème avec mon ordinateur, il ne démarre plus.\n",
      "Sujets : \n",
      " - Sujet  1 : probabilité= 0.047917284\n",
      " - Sujet  2 : probabilité= 0.95208275\n",
      "Document  4 :  Avez-vous essayé de le redémarrer ?\n",
      "Sujets : \n",
      " - Sujet  1 : probabilité= 0.07776173\n",
      " - Sujet  2 : probabilité= 0.92223823\n",
      "Document  5 :  Oui, mais il ne fonctionne toujours pas.\n",
      "Sujets : \n",
      " - Sujet  1 : probabilité= 0.9308633\n",
      " - Sujet  2 : probabilité= 0.06913668\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Charger les documents\n",
    "doc1 = \"Bonjour, pouvez-vous m'aider s'il vous plaît ?\"\n",
    "doc2 = \"Bien sûr, que puis-je faire pour vous ?\"\n",
    "doc3 = \"Je rencontre un problème avec mon ordinateur, il ne démarre plus.\"\n",
    "doc4 = \"Avez-vous essayé de le redémarrer ?\"\n",
    "doc5 = \"Oui, mais il ne fonctionne toujours pas.\"\n",
    "documents = [doc1, doc2, doc3, doc4, doc5]\n",
    "\n",
    "# Tokenisation des documents\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "\n",
    "# Création du dictionnaire\n",
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "\n",
    "# Création du corpus (représentation en sac de mots)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "# Entraînement du modèle LDA\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes=10)\n",
    "\n",
    "# Affichage des sujets les plus probables dans chaque document\n",
    "for i, doc in enumerate(corpus):\n",
    "    print(\"Document \", i+1, \": \", documents[i])\n",
    "    print(\"Sujets : \")\n",
    "    for topic, proba in lda_model.get_document_topics(doc):\n",
    "        print(\" - Sujet \", topic+1, \": probabilité=\", proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea6e32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poids TF-IDF pour le premier document :\n",
      "? : 0.1362711304020637\n",
      "bonjour, : 0.42934401376036485\n",
      "m'aider : 0.42934401376036485\n",
      "plaît : 0.42934401376036485\n",
      "pouvez-vous : 0.42934401376036485\n",
      "s'il : 0.42934401376036485\n",
      "vous : 0.24443561168463546\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Charger les documents\n",
    "doc1 = \"Bonjour, pouvez-vous m'aider s'il vous plaît ?\"\n",
    "doc2 = \"Bien sûr, que puis-je faire pour vous ?\"\n",
    "doc3 = \"Je rencontre un problème avec mon ordinateur, il ne démarre plus.\"\n",
    "doc4 = \"Avez-vous essayé de le redémarrer ?\"\n",
    "doc5 = \"Oui, mais il ne fonctionne toujours pas.\"\n",
    "documents = [doc1, doc2, doc3, doc4, doc5]\n",
    "\n",
    "# Tokenisation des documents\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "\n",
    "# Création du dictionnaire\n",
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "\n",
    "# Création du corpus (représentation en sac de mots)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "# Création du modèle TF-IDF\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "\n",
    "# Application du modèle TF-IDF au corpus\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "\n",
    "# Affichage des poids TF-IDF pour le premier document\n",
    "print(\"Poids TF-IDF pour le premier document :\")\n",
    "for term, weight in tfidf_corpus[0]:\n",
    "    print(dictionary[term], \":\", weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf3eaa8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'lemmatize' from 'gensim.parsing.preprocessing' (C:\\Users\\SACI Thizri\\anaconda3\\lib\\site-packages\\gensim\\parsing\\preprocessing.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lemmatize\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Texte brut à lemmatiser\u001b[39;00m\n\u001b[0;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLes chats sont des animaux adorables et les chiens sont très fidèles.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'lemmatize' from 'gensim.parsing.preprocessing' (C:\\Users\\SACI Thizri\\anaconda3\\lib\\site-packages\\gensim\\parsing\\preprocessing.py)"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import lemmatize\n",
    "\n",
    "# Texte brut à lemmatiser\n",
    "text = \"Les chats sont des animaux adorables et les chiens sont très fidèles.\"\n",
    "\n",
    "# Tokenisation du texte brut en mots\n",
    "words = text.split()\n",
    "\n",
    "# Lemmatisation des mots\n",
    "lemmatized_words = lemmatize(\" \".join(words))\n",
    "\n",
    "# Affichage des mots lemmatisés\n",
    "for word in lemmatized_words:\n",
    "    print(word.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afda5e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les\n",
      "chats\n",
      "sont\n",
      "des\n",
      "animaux\n",
      "adorables\n",
      "et\n",
      "les\n",
      "chiens\n",
      "sont\n",
      "très\n",
      "fidèles\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "# Texte brut à tokenizer\n",
    "text = \"Les chats sont des animaux adorables et les chiens sont très fidèles.\"\n",
    "\n",
    "# Tokenization du texte brut\n",
    "tokens = list(tokenize(text, lowercase=True))\n",
    "\n",
    "# Affichage des tokens\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993d2597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لم\n",
      "يستطع\n",
      "إيجاد\n",
      "حل\n",
      "للمشكلة\n",
      "بالحبل\n",
      "لكنه\n",
      "حاول\n",
      "أن\n",
      "لا\n",
      "يكون\n",
      "قاسيا\n",
      "على\n",
      "نفسه\n",
      "لأن\n",
      "الأطفال\n",
      "السود\n",
      "لا\n",
      "يتلقون\n",
      "تربية\n",
      "جيدة\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "# Texte brut à tokenizer\n",
    "text = \"لم يستطع إيجاد حل للمشكلة بالحبل ، لكنه حاول أن لا يكون قاسياً على نفسه ، لأن الأطفال السود لا يتلقون تربية جيدة .\"\n",
    "\n",
    "# Tokenization du texte brut\n",
    "tokens = list(tokenize(text, lowercase=True))\n",
    "\n",
    "# Affichage des tokens\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869984b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vecteur de 'sentence': [-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193   0.00583312\n",
      "  0.00119818  0.00210273 -0.00411039  0.00722533 -0.00630704  0.00464721\n",
      " -0.00821997  0.00203647 -0.00497705 -0.00424769 -0.00310899  0.00565521\n",
      "  0.0057984  -0.00497465  0.00077333 -0.00849578  0.00780981  0.00925729\n",
      " -0.00274233  0.00080022  0.00074665  0.00547788 -0.00860608  0.00058445\n",
      "  0.00686942  0.00223159  0.00112468 -0.00932216  0.00848237 -0.00626413\n",
      " -0.00299237  0.00349379 -0.00077263  0.00141129  0.00178199 -0.0068289\n",
      " -0.00972481  0.00904058  0.00619805 -0.00691293  0.00340348  0.00020606\n",
      "  0.00475374 -0.00711994  0.00402695  0.00434743  0.00995737 -0.00447374\n",
      " -0.00138927 -0.00731732 -0.00969783 -0.00908026 -0.00102276 -0.00650329\n",
      "  0.00484973 -0.00616403  0.00251919  0.00073944 -0.00339216 -0.00097922\n",
      "  0.00997912  0.00914589 -0.00446183  0.00908303 -0.00564176  0.00593092\n",
      " -0.00309722  0.00343175  0.00301723  0.00690046 -0.00237388  0.00877504\n",
      "  0.00758943 -0.00954765 -0.00800821 -0.0076379   0.00292326 -0.00279472\n",
      " -0.00692952 -0.00812826  0.00830918  0.00199049 -0.00932802 -0.00479272\n",
      "  0.00313674 -0.00471321  0.00528084 -0.00423344  0.00264179 -0.00804569\n",
      "  0.00620989  0.00481889  0.00078719  0.00301345]\n",
      "Mots similaires à 'this': [('so', 0.0679759532213211), ('a', 0.0093911811709404), ('sentence', 0.004503006115555763), ('is', -0.010839185677468777), ('one', -0.02367166243493557)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Exemple de corpus de texte\n",
    "sentences = [[\"this\", \"is\", \"a\", \"sentence\"], [\"so\", \"is\", \"this\", \"one\"]]\n",
    "\n",
    "# Entraînement d'un modèle Word2Vec\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# Récupération des vecteurs de mots pour un mot donné\n",
    "vector = model.wv['sentence']\n",
    "\n",
    "# Recherche des mots similaires à un mot donné\n",
    "similar_words = model.wv.most_similar('this')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Vecteur de 'sentence':\", vector)\n",
    "print(\"Mots similaires à 'this':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638610da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118192911148071)]\n"
     ]
    }
   ],
   "source": [
    "#Exemple de calcul de similarités : \n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load the google word2vec model\n",
    "filename = '/Users/yuyanq/gensim-data/GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
